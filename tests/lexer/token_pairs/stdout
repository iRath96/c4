tests/lexer/token_pairs/input.c:1:1: constant 123
tests/lexer/token_pairs/input.c:1:5: constant 456
tests/lexer/token_pairs/input.c:2:1: constant 0
tests/lexer/token_pairs/input.c:2:2: constant 123
tests/lexer/token_pairs/input.c:3:1: constant 0
tests/lexer/token_pairs/input.c:3:2: identifier abc
tests/lexer/token_pairs/input.c:4:1: constant 0
tests/lexer/token_pairs/input.c:4:2: constant '\''
tests/lexer/token_pairs/input.c:5:1: constant 1
tests/lexer/token_pairs/input.c:5:2: string-literal "\""
tests/lexer/token_pairs/input.c:6:1: constant 100
tests/lexer/token_pairs/input.c:6:4: keyword if
tests/lexer/token_pairs/input.c:7:1: constant 10000
tests/lexer/token_pairs/input.c:7:6: punctuator ...
tests/lexer/token_pairs/input.c:9:1: identifier abc
tests/lexer/token_pairs/input.c:9:5: identifier def
tests/lexer/token_pairs/input.c:10:1: identifier abc
tests/lexer/token_pairs/input.c:10:5: constant 123
tests/lexer/token_pairs/input.c:11:1: identifier abc123
tests/lexer/token_pairs/input.c:12:1: identifier abc
tests/lexer/token_pairs/input.c:12:4: constant '\''
tests/lexer/token_pairs/input.c:13:1: identifier abc
tests/lexer/token_pairs/input.c:13:4: string-literal "\""
tests/lexer/token_pairs/input.c:14:1: identifier abcif
tests/lexer/token_pairs/input.c:15:1: identifier abc
tests/lexer/token_pairs/input.c:15:5: keyword if
tests/lexer/token_pairs/input.c:16:1: identifier abc
tests/lexer/token_pairs/input.c:16:4: punctuator ...
tests/lexer/token_pairs/input.c:18:1: constant '\''
tests/lexer/token_pairs/input.c:18:5: constant 123
tests/lexer/token_pairs/input.c:19:1: constant '\\'
tests/lexer/token_pairs/input.c:19:5: identifier abc
tests/lexer/token_pairs/input.c:20:1: constant '\0'
tests/lexer/token_pairs/input.c:20:5: constant '\''
tests/lexer/token_pairs/input.c:21:1: constant '\xFF'
tests/lexer/token_pairs/input.c:21:7: string-literal "\""
tests/lexer/token_pairs/input.c:22:1: constant '\n'
tests/lexer/token_pairs/input.c:22:5: keyword if
tests/lexer/token_pairs/input.c:23:1: constant '\123'
tests/lexer/token_pairs/input.c:23:7: punctuator ...
tests/lexer/token_pairs/input.c:25:1: string-literal "\""
tests/lexer/token_pairs/input.c:25:5: constant 123
tests/lexer/token_pairs/input.c:26:1: string-literal "\\"
tests/lexer/token_pairs/input.c:26:5: identifier abc
tests/lexer/token_pairs/input.c:27:1: string-literal "\0"
tests/lexer/token_pairs/input.c:27:5: constant '\''
tests/lexer/token_pairs/input.c:28:1: string-literal "\xFF"
tests/lexer/token_pairs/input.c:28:7: string-literal "\""
tests/lexer/token_pairs/input.c:29:1: string-literal "\n"
tests/lexer/token_pairs/input.c:29:5: keyword if
tests/lexer/token_pairs/input.c:30:1: string-literal "\123"
tests/lexer/token_pairs/input.c:30:7: punctuator ...
tests/lexer/token_pairs/input.c:32:1: identifier if123
tests/lexer/token_pairs/input.c:33:1: keyword if
tests/lexer/token_pairs/input.c:33:4: constant 123
tests/lexer/token_pairs/input.c:34:1: keyword if
tests/lexer/token_pairs/input.c:34:3: constant '\''
tests/lexer/token_pairs/input.c:35:1: keyword if
tests/lexer/token_pairs/input.c:35:3: string-literal "\""
tests/lexer/token_pairs/input.c:36:1: identifier ifif
tests/lexer/token_pairs/input.c:37:1: keyword if
tests/lexer/token_pairs/input.c:37:4: keyword if
tests/lexer/token_pairs/input.c:38:1: keyword if
tests/lexer/token_pairs/input.c:38:3: punctuator ...
tests/lexer/token_pairs/input.c:40:1: punctuator ...
tests/lexer/token_pairs/input.c:40:4: constant 123
tests/lexer/token_pairs/input.c:41:1: punctuator %:%:
tests/lexer/token_pairs/input.c:41:5: identifier abc
tests/lexer/token_pairs/input.c:42:1: punctuator >>=
tests/lexer/token_pairs/input.c:42:4: constant '\''
tests/lexer/token_pairs/input.c:43:1: punctuator :>
tests/lexer/token_pairs/input.c:43:3: string-literal "\""
tests/lexer/token_pairs/input.c:44:1: punctuator ++
tests/lexer/token_pairs/input.c:44:3: keyword if
tests/lexer/token_pairs/input.c:45:1: punctuator %:
tests/lexer/token_pairs/input.c:45:3: punctuator ++
